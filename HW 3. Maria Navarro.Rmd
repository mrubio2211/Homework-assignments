---
title: "HW 3"
author: "Maria Navarro"
date: "9/24/2024"
output:
  html_document:
    number_sections: true
  pdf_document: default
---

#

Let $E[X] = \mu$.  Show that $Var[X] := E[(X-E[X])^2] = E[X^2]-(E[X])^2$.  Note, all you have to do is show the second equality (the first is our definition from class). 



# 

In the computational section of this homework, we will discuss support vector machines and tree-based methods.  I will begin by simulating some data for you to use with SVM. 

```{r}
library(e1071)
set.seed(1) 
x=matrix(rnorm(200*2),ncol=2)
x[1:100,]=x[1:100,]+2
x[101:150,]=x[101:150,]-2
y=c(rep(1,150),rep(2,50))
dat=data.frame(x=x,y=as.factor(y))
plot(x, col=y)

```


##

Quite clearly, the above data is not linearly separable.  Create a training-testing partition with 100 random observations in the training partition.  Fit an svm on this training data using the radial kernel, and tuning parameters $\gamma=1$, cost $=1$.  Plot the svm on the training data.  

```{r}
set.seed(1)
library(e1071)

set.seed(1)

x = matrix(rnorm(200 * 2), ncol = 2)
x[1:100,] = x[1:100,] + 2
x[101:150,] = x[101:150,] - 2
y = c(rep(1, 150), rep(2, 50))
dat = data.frame(x = x, y = as.factor(y))

train_indices = sample(1:nrow(dat), 100)
train_data = dat[train_indices,]
test_data = dat[-train_indices,]

svm_model = svm(y ~ ., data = train_data, kernel = "radial", gamma = 1, cost = 1)

plot(svm_model, train_data)

```

##

Notice that the above decision boundary is decidedly non-linear.  It seems to perform reasonably well, but there are indeed some misclassifications.  Let's see if increasing the cost ^[Remember this is a parameter that decides how smooth your decision boundary should be] helps our classification error rate.  Refit the svm with the radial kernel, $\gamma=1$, and a cost of 10000.  Plot this svm on the training data. 

```{r}

svm_model_high_cost = svm(y ~ ., data = train_data, kernel = "radial", gamma = 1, cost = 10000)

plot(svm_model_high_cost, train_data)

```

##

It would appear that we are better capturing the training data, but comment on the dangers (if any exist), of such a model. 

It seems to be better capturing the training data by increasing the cost of the SVM model, which may decrease the error rates for this particular sample. When the model loses its capacity to generalize on unobserved data IT IS because it overfits the training set. This implies that even if the model performs well on the training set, it might not be able to accurately identify new data, which would reduce its practical usefulness.

##

Create a confusion matrix by using this svm to predict on the current testing partition.  Comment on the confusion matrix.  Is there any disparity in our classification results?    

```{r, eval = FALSE}

table(true=dat[-train,"y"], pred=predict(svmfit, newdata=dat[-train,]))
```

predictions = predict(svm_model_high_cost, newdata = test_data)

confusion_matrix = table(true = test_data$y, pred = predictions)

print(confusion_matrix)

    pred
true  1  2
   1 57 20
   2  5 18
   
False positives, false negatives, true positives, and true negatives can all be distinguished. A large difference between the classes could mean that the model is biased toward one of the classes and is having trouble classifying it accurately. In order to evaluate the model's efficacy and make any required modifications, it is imperative to take these indicators into account.


##

Is this disparity because of imbalance in the training/testing partition?  Find the proportion of class `2` in your training partition and see if it is broadly representative of the underlying 25\% of class 2 in the data as a whole.  

```{r}

train_class_proportion = sum(train_data$y == 2) / nrow(train_data)

overall_class_proportion = sum(dat$y == 2) / nrow(dat)

cat("Proportion of class 2 in the training partition:", train_class_proportion, "\n")
cat("Proportion of class 2 in the entire dataset:", overall_class_proportion, "\n")



```

It is slightly higher than the 25% proportion in the complete dataset. This implies that there isn't a substantial imbalance in the training partition causing the discrepancy in classification results. Nevertheless, it is crucial to keep an eye on these ratios and take into account additional factors that could lead to incorrect classifications.

##

Let's try and balance the above to solutions via cross-validation.  Using the `tune` function, pass in the training data, and a list of the following cost and $\gamma$ values: {0.1, 1, 10, 100, 1000} and {0.5, 1,2,3,4}.  Save the output of this function in a variable called `tune.out`.  

```{r}

set.seed(1)
library(e1071)

cost_values = c(0.1, 1, 10, 100, 1000)
gamma_values = c(0.5, 1, 2, 3, 4)

tune.out = tune(svm, y ~ ., data = train_data,
                ranges = list(cost = cost_values, gamma = gamma_values))

print(tune.out)


```

I will take `tune.out` and use the best model according to error rate to test on our data.  I will report a confusion matrix corresponding to the 100 predictions.  


```{r, eval = FALSE}
table(true=dat[-train,"y"], pred=predict(tune.out$best.model, newdata=dat[-train,]))
```

predictions_best_model = predict(tune.out$best.model, newdata = test_data)

confusion_matrix_best_model = table(true = test_data$y, pred = predictions_best_model)

print(confusion_matrix_best_model)

##

Comment on the confusion matrix.  How have we improved upon the model in question 2 and what qualifications are still necessary for this improved model.  

    pred
true  1  2
   1 67 10
   2  2 21
   
With 67 true positives and just 10 erroneous negatives for class {1} and 21 true positives and 2 false negatives for class {2}, the confusion matrix indicates an improvement in the model's performance. The fact that there are still misclassifications suggests that the model is not flawless, even though the error rate has decreased. It would be beneficial to adjust the settings, gather more information, and assess other metrics like precision and recall in order to make it even better.

# 
Let's turn now to decision trees.  

```{r}

library(kmed)
library(tree)

data(heart)

heart$heart_disease_binary <- ifelse(heart$class == 0, 0, 1)

heart$heart_disease_binary <- as.factor(heart$heart_disease_binary)

str(heart)

tree_model = tree(heart_disease_binary ~ ., data = heart)

plot(tree_model)
text(tree_model, pretty = 0)


```

## 

Train a classification tree on a 240 observation training subset (using the seed I have set for you).  Plot the tree.  

```{r}
set.seed(101)

library(tree)

set.seed(101)

train_indices = sample(1:nrow(heart), size = 240)
train_data = heart[train_indices, ]

tree_model = tree(heart_disease_binary ~ ., data = train_data)

summary(tree_model)

plot(tree_model)
text(tree_model, pretty = 0) 

```


## 

Use the trained model to classify the remaining testing points.  Create a confusion matrix to evaluate performance.  Report the classification error rate.  

```{r}

# Prepare the test data
test_data = heart[-train_indices, ]

# Use the trained model to predict on the test data
predictions = predict(tree_model, newdata = test_data, type = "class")

# Create a confusion matrix
confusion_matrix = table(True = test_data$heart_disease_binary, Predicted = predictions)

# Print the confusion matrix
print(confusion_matrix)

# Calculate the classification error rate
classification_error_rate = 1 - sum(diag(confusion_matrix)) / sum(confusion_matrix)

# Print the classification error rate
cat("Classification error rate:", classification_error_rate, "\n")

```
The percentage of inaccurate predictions made in relation to all observations is known as the classification error rate. The error rate in this instance is roughly 0.13, meaning that 13% of the classification tree model's predictions do not match the actual classes. This result indicates that the model is performing reasonably, although it can yet be enhanced.

##  

Above we have a fully grown (bushy) tree.  Now, cross validate it using the `cv.tree` command.  Specify cross validation to be done according to the misclassification rate.  Choose an ideal number of splits, and plot this tree.  Finally, use this pruned tree to test on the testing set.  Report a confusion matrix and the misclassification rate.  

```{r}

library(tree)

set.seed(101)

tree_model = tree(heart_disease_binary ~ ., data = train_data)

cv_result = cv.tree(tree_model, FUN = prune.tree)

plot(cv_result$size, cv_result$dev, type = "b", 
     xlab = "Number of Terminal Nodes", 
     ylab = "Deviance", 
     main = "Cross-Validation for Tree Pruning")

best_size = cv_result$size[which.min(cv_result$dev)]

pruned_tree = prune.tree(tree_model, best = best_size)

if (!is.null(pruned_tree)) {
   
    plot(pruned_tree)
    text(pruned_tree, pretty = 0)  

    predictions_pruned = predict(pruned_tree, newdata = test_data, type = "class")

    confusion_matrix_pruned = table(True = test_data$heart_disease_binary, Predicted = predictions_pruned)

    print(confusion_matrix_pruned)

    misclassification_rate_pruned = 1 - sum(diag(confusion_matrix_pruned)) / sum(confusion_matrix_pruned)

    cat("Classification error rate of the pruned tree:", misclassification_rate_pruned, "\n")
} else {
    cat("The pruned tree is NULL or was not created successfully.\n")
}


```


##

Discuss the trade-off in accuracy and interpretability in pruning the above tree. 

A classification tree's accuracy and interpretability must be traded off during pruning. A fully developed tree may be able to identify intricate patterns in the data, improving training set accuracy. But it can also lead to overfitting, in which case the model does not work well with new data. On the other hand, a simplified tree may lose some accuracy but improves interpretability and generalization to new data. Therefore, striking the correct balance is essential, since a more straightforward model can frequently yield more useful insights without making significant performance sacrifices.

## 

Discuss the ways a decision tree could manifest algorithmic bias.  

Unrepresentative training data can lead to algorithmic bias in a decision tree, reinforcing preexisting biases in predictions. Features that disproportionately affect judgments can also be biased, producing skewed divides. In addition, imbalanced classes may cause the tree to favor the majority class, while overfitting may capture biases and noise in the training set. To make accurate predictions, these problems must be resolved.